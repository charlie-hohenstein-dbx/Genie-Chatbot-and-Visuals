# Building an Intelligent Data Chatbot with Databricks MCP, Model Serving, and LangGraph

## Turn Natural Language Questions into Interactive Visualizations in Minutes

![Hero Image Placeholder: Screenshot of chatbot generating a chart from natural language]

---

## Introduction

Imagine asking your data a simple question like *"Show me sales by region as a bar chart"* and instantly getting back both the data **and** an interactive visualizationâ€”all powered by AI. No SQL required. No manual chart configuration. Just conversation.

In this post, I'll show you how to build a production-ready data chatbot using:
- **Databricks Genie MCP** for natural language data queries
- **Unity Catalog Functions MCP** as AI-callable tools
- **Databricks Model Serving** with Claude 3.7 Sonnet
- **LangGraph** for robust agent orchestration

By the end, you'll have a fully functional chatbot that can query your data warehouse, generate charts, and display resultsâ€”all through a clean Streamlit interface.

---

## The Problem: Bridging the Gap Between Users and Data

Data teams face a common challenge: business users want quick insights, but querying data requires SQL knowledge, and creating visualizations requires even more technical skill. Traditional BI tools help, but they still have a learning curve.

**What if users could just ask questions in plain English?**

This is where AI agents come in. But building a production-ready agent that can:
1. Understand natural language queries
2. Execute SQL against your data warehouse
3. Transform results into visualizations
4. Handle errors gracefully
5. Maintain conversation context


---

## The Solution: Databricks MCP + LangGraph

Databricks recently launched **Model Context Protocol (MCP) servers**, which act as bridges between AI models and enterprise data/tools. Combined with **LangGraph** for stateful agent workflows, we can build a robust, production-ready solution.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Query                           â”‚
â”‚        "Show me trips per year as a bar chart"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Streamlit Chat Interface                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LangGraph Agent                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Databricks Model Serving                        â”‚   â”‚
â”‚  â”‚  (Claude 3.7 Sonnet)                             â”‚   â”‚
â”‚  â”‚  - Understands user intent                       â”‚   â”‚
â”‚  â”‚  - Decides which tools to call                   â”‚   â”‚
â”‚  â”‚  - Orchestrates multi-step workflows             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                   â”‚
              â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Genie MCP Server    â”‚  â”‚  UC Functions MCP Server â”‚
â”‚  - Natural language  â”‚  â”‚  - genie_to_chart()      â”‚
â”‚    to SQL            â”‚  â”‚  - Transforms data       â”‚
â”‚  - Query execution   â”‚  â”‚  - Generates Plotly JSON â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

1. **Genie MCP Server**: Translates natural language to SQL and executes queries against your Databricks data warehouse
2. **UC Functions MCP Server**: Exposes Unity Catalog functions as AI-callable tools
3. **LangGraph Agent**: Orchestrates tool calls, maintains state, and handles conversation flow
4. **Databricks Model Serving**: Hosts the LLM (Claude 3.7 Sonnet) that drives the agent

---

## Prerequisites & Setup

Before building the agent, we need to set up three key components in Databricks:

### 1. Create a Genie Space

Genie is Databricks' AI-powered analytics assistant that translates natural language to SQL. First, we'll create a Genie Space connected to your data.

**Steps:**

1. **Navigate to Genie** in your Databricks workspace:
   - Click on "Genie" in the left sidebar
   - Click "Create Space"

2. **Configure Your Space:**
   - **Name**: Give it a descriptive name (e.g., "NYC Taxi Analytics")
   - **Description**: Explain what data this space contains
   - **Data Source**: Select your Unity Catalog tables or SQL warehouse

   ![Screenshot Placeholder: Genie Space Creation]

3. **Add Instructions** (Optional but Recommended):
   - Help Genie understand your data schema
   - Example: *"This dataset contains NYC taxi trip data from 2016. The `tpep_pickup_datetime` field is the pickup timestamp. Use this field for time-based analysis."*

4. **Test Your Space:**
   - Ask a simple question: *"How many trips are in the dataset?"*
   - Verify Genie generates correct SQL

5. **Copy the Space ID:**
   - Once created, navigate to your Genie Space
   - Copy the Space ID from the URL:
   ```
   https://<workspace>.databricks.com/genie/rooms/01f0ab8c079d17b8a00584e70d2ac18c/...
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                 This is your GENIE_SPACE_ID
   ```

**Important:** Save this Space IDâ€”you'll need it to configure the MCP server.

---

### 2. Set Up Model Serving Endpoint

Databricks Model Serving hosts LLMs that can be called via OpenAI-compatible APIs. We'll use Claude 3.7 Sonnet for its excellent tool-calling capabilities.

**Option A: Use Foundation Model APIs (Recommended)**

Databricks provides pre-configured endpoints for popular models:

1. **Navigate to Model Serving:**
   - Click "Serving" in the left sidebar
   - Select "Foundation Model APIs"

2. **Enable Claude 3.7 Sonnet:**
   - Find "Claude 3.7 Sonnet" in the list
   - Click "Enable"
   - Endpoint name: `databricks-claude-3-7-sonnet`

3. **Test the Endpoint:**
   ```python
   from databricks.sdk import WorkspaceClient
   
   ws = WorkspaceClient()
   client = ws.serving_endpoints.get_open_ai_client()
   
   response = client.chat.completions.create(
       model="databricks-claude-3-7-sonnet",
       messages=[{"role": "user", "content": "Hello!"}]
   )
   print(response.choices[0].message.content)
   ```

**Option B: External Model (If Foundation Models Not Available)**

If your workspace doesn't have Foundation Model APIs enabled:

1. **Create an External Model Serving Endpoint:**
   - Navigate to "Serving" â†’ "Create Serving Endpoint"
   - Choose "External Model"
   - Configure with your API provider (OpenAI, Anthropic, etc.)

2. **Use the endpoint name in your code:**
   ```python
   llm_endpoint = "your-external-model-endpoint"
   ```

**Recommended Models:**
- âœ… **Claude 3.7 Sonnet**: Best tool calling, most reliable
- âœ… **Claude 3.5 Sonnet**: Good balance of speed and capability
- âš ï¸ **Llama 3.1 405B**: Good but slower
- âŒ **Smaller models** (<70B params): May struggle with complex tool calling

---

### 3. Configure Unity Catalog

Ensure you have a catalog and schema for your UC functions:

1. **Create Catalog** (if needed):
   ```sql
   CREATE CATALOG IF NOT EXISTS chat_app_demo;
   ```

2. **Create Schema** (if needed):
   ```sql
   CREATE SCHEMA IF NOT EXISTS chat_app_demo.dev;
   ```

3. **Grant Permissions:**
   ```sql
   -- Grant yourself CREATE FUNCTION permission
   GRANT CREATE FUNCTION ON SCHEMA chat_app_demo.dev TO `your_user@company.com`;
   
   -- Grant EXECUTE to users who will use the chatbot
   GRANT EXECUTE ON SCHEMA chat_app_demo.dev TO `your_group`;
   ```

4. **Verify Access:**
   ```sql
   SHOW FUNCTIONS IN chat_app_demo.dev;
   ```

---

### 4. Set Up Your Development Environment

Install required packages:

```bash
pip install databricks-sdk streamlit pandas plotly \
    unitycatalog-ai langchain langgraph langchain-core \
    databricks-mcp python-dotenv nest-asyncio
```

Create a `.env` file with your credentials:

```bash
# .env
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=dapi...  # Create a PAT in User Settings â†’ Access Tokens
GENIE_SPACE_ID=01f0ab8c079d17b8a00584e70d2ac18c  # From Step 1
```

**Security Note:** Never commit `.env` files to Git. Add to `.gitignore`:

```bash
echo ".env" >> .gitignore
```

---

## Implementation: Step-by-Step Guide

Now that we have Genie, Model Serving, and Unity Catalog configured, let's build the chatbot.

### Step 1: Create a Unity Catalog Function for Chart Generation

Unity Catalog functions become AI-callable tools when exposed via MCP. Databricks automatically exposes these functions via MCP. Let's create a function that transforms Genie's JSON output into Plotly charts.

```python
from unitycatalog.ai.core.databricks import DatabricksFunctionClient

def genie_to_chart(genie_response_json: str, chart_type: str) -> str:
    """
    Transform Genie MCP response into a Plotly chart.
    
    Args:
        genie_response_json: Raw JSON string from Genie MCP query_space tool
        chart_type: Type of chart - "bar", "line", or "pie"
    
    Returns:
        JSON with plotly_json and chart_type keys
    """
    import json
    import plotly.express as px
    import pandas as pd
    
    # Parse Genie response (handles double-nested JSON)
    genie_data = json.loads(genie_response_json)
    if 'content' in genie_data:
        content_data = json.loads(genie_data['content'])
    else:
        content_data = genie_data
    
    # Extract data from Genie's response structure
    columns_info = content_data['statement_response']['manifest']['schema']['columns']
    column_names = [col['name'] for col in columns_info]
    
    data_array = content_data['statement_response']['result']['data_array']
    rows = [[value['string_value'] for value in row['values']] for row in data_array]
    
    # Create DataFrame
    df = pd.DataFrame(rows, columns=column_names)
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='ignore')
    
    # Smart column selection
    string_cols = df.select_dtypes(include=['object', 'string']).columns
    numeric_cols = df.select_dtypes(include='number').columns
    
    x_col = string_cols[0] if len(string_cols) > 0 else df.columns[0]
    y_col = numeric_cols[-1] if len(numeric_cols) > 0 else df.columns[-1]
    
    # Generate chart based on type
    chart_functions = {
        "bar": lambda: px.bar(df, x=x_col, y=y_col, title=f"{y_col} by {x_col}"),
        "line": lambda: px.line(df, x=x_col, y=y_col, title=f"{y_col} by {x_col}", markers=True),
        "pie": lambda: px.pie(df, names=x_col, values=y_col, title=f"{y_col} by {x_col}")
    }
    
    fig = chart_functions[chart_type.lower()]()
    
    # Return structured JSON
    return json.dumps({
        "plotly_json": json.loads(fig.to_json()),
        "chart_type": chart_type.lower()
    })

# Deploy to Unity Catalog
uc_client = DatabricksFunctionClient()

uc_client.create_python_function(
    func=genie_to_chart,
    catalog="chat_app_demo",
    schema="dev",
    replace=True
)

```

**Why This Works:**
- The function signature is automatically converted to an MCP tool schema
- Docstrings become tool descriptions for the LLM
- The LLM sees this as `chat_app_demo__dev__genie_to_chart` (MCP naming convention)

### Step 2: Build the LangGraph Agent

LangGraph provides a stateful workflow for agent execution. Here's the core implementation:

```python
# databricks_chat_app/langgraph_agent.py

from typing import Annotated, Sequence, TypedDict
from databricks.sdk import WorkspaceClient
from databricks_mcp import DatabricksMCPClient
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langgraph.graph import END, StateGraph
from langgraph.prebuilt.tool_node import ToolNode

class AgentState(TypedDict):
    """State of the agent conversation"""
    messages: Annotated[Sequence[BaseMessage], add_messages]

class LangGraphMCPAgent:
    def __init__(
        self, 
        workspace_client: WorkspaceClient, 
        genie_space_id: str,
        llm_endpoint: str = "databricks-claude-3-7-sonnet"
    ):
        self.workspace_client = workspace_client
        self.llm_endpoint = llm_endpoint
        
        # Configure MCP server URLs
        host = workspace_client.config.host
        self.managed_mcp_urls = [
            f"{host}/api/2.0/mcp/genie/{genie_space_id}",
            f"{host}/api/2.0/mcp/functions/chat_app_demo/dev",
        ]
        
        # Discover tools from MCP servers
        self.tools = self._load_mcp_tools()
        
        # Build the LangGraph workflow
        self.agent = self._build_agent()
    
    def _build_agent(self):
        """Build the LangGraph agent workflow"""
        
        def should_continue(state: AgentState):
            """Decide whether to continue to tools or end"""
            last_message = state["messages"][-1]
            if isinstance(last_message, AIMessage) and last_message.tool_calls:
                return "continue"
            return "end"
        
        def call_model(state: AgentState):
            """Call the LLM with tool definitions"""
            messages = state["messages"]
            
            system_prompt = """You are a data assistant.
            
            WORKFLOW:
            1. Use query_space to get data from Genie
            2. Use genie_to_chart to create visualizations
            3. Provide a brief summary
            """
            
            # Format for OpenAI API
            full_messages = [
                {"role": "system", "content": system_prompt}
            ] + [
                {"role": "user" if isinstance(m, HumanMessage) else "assistant", 
                 "content": m.content}
                for m in messages
            ]
            
            # Call LLM with tools
            response = self.workspace_client.serving_endpoints.get_open_ai_client().chat.completions.create(
                model=self.llm_endpoint,
                messages=full_messages,
                tools=self._format_tools(),
                tool_choice="auto"
            )
            
            # Convert to AIMessage
            ai_msg = AIMessage(content=response.choices[0].message.content or "")
            if response.choices[0].message.tool_calls:
                ai_msg.tool_calls = [...]  # Convert tool calls
            
            return {"messages": [ai_msg]}
        
        def call_tools(state: AgentState):
            """Execute tools via MCP"""
            tool_node = ToolNode(self.tools)
            return tool_node.invoke(state)
        
        # Build the graph
        workflow = StateGraph(AgentState)
        workflow.add_node("agent", call_model)
        workflow.add_node("tools", call_tools)
        workflow.set_entry_point("agent")
        
        # Add conditional routing
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {"continue": "tools", "end": END}
        )
        workflow.add_edge("tools", "agent")
        
        return workflow.compile()
    
    def chat(self, user_message: str) -> dict:
        """Process a user message"""
        initial_state = {"messages": [HumanMessage(content=user_message)]}
        result = self.agent.invoke(initial_state, config={"recursion_limit": 20})
        
        return {
            "response": result["messages"][-1].content,
            "messages": result["messages"],
            "error": None
        }
```

**Key Design Decisions:**

1. **State Management**: LangGraph's `AgentState` tracks the full conversation history
2. **Conditional Routing**: `should_continue` decides whether to call more tools or end
3. **Tool Discovery**: MCP servers expose their tools dynamicallyâ€”no hardcoding
4. **Recursion Limit**: Prevents infinite loops if the agent gets confused

### Step 3: Create the Streamlit Interface

```python
# databricks_chat_app/app.py

import streamlit as st
import pandas as pd
import plotly.io as pio
from databricks.sdk import WorkspaceClient
from databricks_chat_app.langgraph_agent import LangGraphMCPAgent

# Initialize agent
workspace_client = WorkspaceClient()
agent = LangGraphMCPAgent(
    workspace_client=workspace_client,
    genie_space_id="your-genie-space-id"
)

st.title("ğŸ¤– Data Chatbot with Genie MCP")

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Display charts if present
        if message.get("charts"):
            for chart_info in message["charts"]:
                with st.expander(f"{chart_info['chart_type'].capitalize()} Chart", expanded=True):
                    fig = pio.from_json(json.dumps(chart_info["plotly_json"]))
                    st.plotly_chart(fig, use_container_width=True)

# Chat input
if prompt := st.chat_input("Ask about your data..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Call agent
    response = agent.chat(prompt)
    
    # Parse charts from tool results
    charts = []
    for msg in response["messages"]:
        if hasattr(msg, 'content') and isinstance(msg.content, str):
            try:
                data = json.loads(msg.content)
                if "plotly_json" in data:
                    charts.append(data)
            except:
                pass
    
    st.session_state.messages.append({
        "role": "assistant",
        "content": response["response"],
        "charts": charts
    })
    
    st.rerun()
```

---

## How It Works: A Real Example

Let's trace through what happens when a user asks: *"Show me trips per year as a bar chart"*

### Step 1: User Input â†’ LangGraph Agent

The Streamlit app sends the message to the LangGraph agent, which initializes the state:

```python
state = {"messages": [HumanMessage("Show me trips per year as a bar chart")]}
```

### Step 2: Agent Node â†’ LLM Decision

The agent node calls Claude 3.7 Sonnet with:
- System prompt explaining the workflow
- All available tools (from MCP servers)
- User's message

**LLM's Decision:**
> "The user wants trip data and a chart. I'll first call `query_space_01f0ab8c...` to get the data."

**Tool Call:**
```json
{
  "name": "query_space_01f0ab8c079d17b8a00584e70d2ac18c",
  "args": {"query": "Count trips per year from NYC taxi data"}
}
```

### Step 3: Tools Node â†’ Genie MCP Execution

The tools node receives the tool call and executes it via the Genie MCP server:

```python
mcp_client = DatabricksMCPClient(server_url=genie_mcp_url)
response = mcp_client.call_tool("query_space_...", {"query": "..."})
```

**Genie's Response:**
```json
{
  "content": "{\"query\": \"SELECT YEAR(tpep_pickup_datetime) AS year, COUNT(*) AS trip_count FROM samples.nyctaxi.trips GROUP BY year\", \"statement_response\": {...}}",
  "conversationId": "...",
  "messageId": "..."
}
```

### Step 4: Back to Agent Node

The graph loops back to the agent node with the updated state:

```python
state = {
  "messages": [
    HumanMessage("Show me trips per year..."),
    AIMessage(tool_calls=[...]),
    ToolMessage(content='{"query": "...", "statement_response": {...}}')
  ]
}
```

**LLM's Next Decision:**
> "Great! I have the data. Now I'll call `chat_app_demo__dev__genie_to_chart` to create the bar chart."

**Tool Call:**
```json
{
  "name": "chat_app_demo__dev__genie_to_chart",
  "args": {
    "genie_response_json": "{\"query\": \"...\", \"statement_response\": {...}}",
    "chart_type": "bar"
  }
}
```

### Step 5: UC Function Execution

The UC function MCP server executes `genie_to_chart`:
1. Parses the nested JSON from Genie
2. Extracts columns and data
3. Creates a pandas DataFrame
4. Generates a Plotly bar chart
5. Returns JSON with `plotly_json` and `chart_type`

### Step 6: Final Response

The LLM sees the chart was successfully created and generates a summary:

```
"Here's a bar chart showing the number of trips per year from the NYC taxi dataset."
```

The Streamlit app parses the Plotly JSON from the tool messages and renders it using `st.plotly_chart()`.

**Total Agent Iterations:** 3 (query data â†’ create chart â†’ summarize)

---

## Why This Architecture Works

### 1. **Dynamic Tool Discovery**

The agent doesn't hardcode tool names. MCP servers expose their tools at runtime:

```python
mcp_client.list_tools()
# Returns:
# - query_space_01f0ab8c079d17b8a00584e70d2ac18c
# - poll_response_01f0ab8c079d17b8a00584e70d2ac18c
# - chat_app_demo__dev__genie_to_chart
```

**Benefit:** Add new UC functions â†’ agent discovers them automatically.

### 2. **LLM-Driven Orchestration**

The agent doesn't use brittle if/else logic. The LLM reads tool descriptions and decides:
- Which tools to call
- In what order
- What arguments to pass

**Benefit:** Handles complex, multi-step queries intelligently.

### 3. **Separation of Concerns**

- **Genie MCP**: Handles data queries (SQL generation + execution)
- **UC Functions**: Handle data transformation (JSON â†’ Plotly)
- **LangGraph**: Handles orchestration (tool calling + state management)
- **Streamlit**: Handles UI (chat interface + chart rendering)

**Benefit:** Each component is testable and maintainable independently.

### 4. **State Management**

LangGraph tracks the full conversation history:
- Previous queries
- Tool calls
- Tool results

**Benefit:** The agent can handle follow-up questions like "Now show it as a pie chart."

---

## Key Takeaways

### What We Built

âœ… A production-ready data chatbot that:
- Understands natural language queries
- Executes SQL via Genie
- Generates interactive visualizations
- Maintains conversation context
- Handles errors gracefully

### Technologies Used

- **Databricks Genie MCP**: Natural language to SQL
- **Unity Catalog Functions**: AI-callable data transformations
- **Databricks Model Serving**: Hosted LLM (Claude 3.7 Sonnet)
- **LangGraph**: Stateful agent orchestration
- **Streamlit**: Chat UI

### Performance Characteristics

- **Latency**: 3-5 seconds for simple queries (network + LLM + query execution)
- **Scalability**: Leverages Databricks serverless compute
- **Cost**: Pay-per-token for model serving + query compute

---

## Next Steps & Extensions

### 1. **Add More Chart Types**

Extend `genie_to_chart` to support:
- Scatter plots
- Heatmaps
- Multi-series line charts

### 2. **Implement Streaming**

Use LangGraph's `.stream()` for real-time updates:

```python
for chunk in agent.stream(initial_state):
    st.write(chunk)  # Show agent's thinking process
```

### 3. **Add Memory/Checkpointing**

Enable the agent to remember past conversations:

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
workflow.compile(checkpointer=checkpointer)
```

### 4. **Human-in-the-Loop**

Add approval steps for destructive operations:

```python
workflow.add_node("human_approval", ask_for_approval)
workflow.add_conditional_edges("tools", needs_approval, {
    "yes": "human_approval",
    "no": "agent"
})
```

### 5. **Deploy to Databricks Apps**

Use Databricks Asset Bundles for production deployment:

```bash
databricks bundle deploy
```

---

## Conclusion

Building intelligent agents is no longer just an experimentâ€”it's becoming production reality. By combining:

- **Databricks MCP** for secure, governed data access
- **Unity Catalog Functions** for reusable, AI-callable tools
- **Model Serving** for scalable LLM inference
- **LangGraph** for robust orchestration

...you can build chatbots that turn natural language into actionable insights in minutes, not months.

The code for this project is [available on GitHub](#). Try it out, extend it, and let me know what you build!

---

## Resources

- **Databricks MCP Documentation**: [docs.databricks.com/mcp](https://docs.databricks.com/aws/en/generative-ai/mcp/managed-mcp.html)
- **Unity Catalog AI**: [docs.unitycatalog.io/ai](https://docs.unitycatalog.io/ai/client/)
- **LangGraph Guide**: [langchain-ai.github.io/langgraph](https://langchain-ai.github.io/langgraph/)
- **LangGraph MCP Agent Example**: [Databricks Notebook](https://docs.databricks.com/aws/en/notebooks/source/generative-ai/langgraph-mcp-tool-calling-agent.html)

---

**About the Author**

[Your name] is a [Your title] at Databricks, focused on making AI accessible to data teams. Connect on [LinkedIn/Twitter].

---

*Did you find this helpful? Follow [@databricks](https://medium.com/@databricks) on Medium for more AI and data engineering content.*

